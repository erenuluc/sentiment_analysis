{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the path to your CSV file\n",
    "file_path = 'train_file.csv'\n",
    "\n",
    "# Read the CSV file using pandas, specifying the delimiter as ';'\n",
    "train_df = pd.read_csv(file_path, delimiter=';')\n",
    "\n",
    "# Specify the path to your CSV file\n",
    "file_path = 'test_file.csv'\n",
    "\n",
    "# Read the CSV file using pandas, specifying the delimiter as ';'\n",
    "test_df = pd.read_csv(file_path, delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "base_prompt = \"<s>[INST]\\n<<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n{user_prompt}[/INST]\"\n",
    "\n",
    "access_token = \"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\"\n",
    "\n",
    "tokenizer = None\n",
    "pipeline = None\n",
    "# Uncomment to download and run\n",
    "model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model, token=access_token)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map=\"auto\",\n",
    "    token=access_token,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Format(input):\n",
    "    if input.find(\"negative\") != -1:\n",
    "        return \"negative\"\n",
    "\n",
    "    elif input.find(\"neutral\") != -1:\n",
    "        return \"neutral\"\n",
    "\n",
    "    elif input.find(\"positive\") != -1:\n",
    "        return \"positive\"\n",
    "\n",
    "    return \"Error\"\n",
    "\n",
    "def Generate_Few_Shot_llama (train_df, test_df, shot_num, retrieval_technique):\n",
    "\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    misclassifications = []\n",
    "\n",
    "    for idx, testrow in test_df.iterrows():\n",
    "\n",
    "        user_prompt = \"\"\n",
    "        \n",
    "        event = \"A chat between a user and an artificial intelligence assistant. The assistant gives one word reply to the user's question.\"\n",
    "        task = \"Given the sentence from GitHub, perform sentiment classification task. Only return 'negative', 'neutral' or 'positive'\"\n",
    "        guess = testrow[2]\n",
    "\n",
    "        retrieved_examples = retrieval_technique(guess, shot_num)\n",
    "\n",
    "        for i in retrieved_examples:\n",
    "            user_prompt += \"Example Sentence: \"+ train_df['Text'][i] + \"Label: \" +train_df['Polarity'][i]\n",
    "        \n",
    "        prompt = f\"<s>[INST] <<SYS>>\\n{event}\\n<</SYS>>\\n{task}\\n{user_prompt}[/INST]\\nSure!\\n</s><s>[INST]\\nSentence: {guess}\\nLabel:\\n[/INST]\\n\"\n",
    "\n",
    "        # Generate the natural status description using Llama 2\n",
    "        generated_text = pipeline(prompt, do_sample=True, max_new_tokens=20, num_return_sequences=1)[0]['generated_text']\n",
    "\n",
    "        output = generated_text.strip()\n",
    "\n",
    "        # Find the index of \"Label:\"\n",
    "        index_label = output.rfind(\"]\")\n",
    "\n",
    "        # Extract substring after \"Label:\"\n",
    "        substring_after_label = output[index_label + 1:].strip()\n",
    "\n",
    "        predicted_label = Format(substring_after_label.lower())\n",
    "        true_label = testrow[1]\n",
    "        base = []\n",
    "\n",
    "        if predicted_label != true_label:\n",
    "            base.append(guess)\n",
    "            base.append(true_label)\n",
    "            base.append(predicted_label)\n",
    "            misclassifications.append(base)\n",
    "\n",
    "        predicted_labels.append(predicted_label)\n",
    "        true_labels.append(true_label)\n",
    "\n",
    "        if idx % 50 == 0:\n",
    "            print(idx)\n",
    "    \n",
    "    return true_labels, predicted_labels, misclassifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "\n",
    "def Result (true_labels_pre, predicted_labels_pre):\n",
    "\n",
    "    true_labels = true_labels_pre\n",
    "    predicted_labels = predicted_labels_pre\n",
    "\n",
    "    for idx, label in enumerate(predicted_labels):\n",
    "        if label == \"Error\":\n",
    "            del predicted_labels[idx]\n",
    "            del true_labels[idx]\n",
    "\n",
    "    conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "\n",
    "    class_rep = classification_report(true_labels, predicted_labels, zero_division=0)\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(class_rep)\n",
    "\n",
    "    # Calculate the F1-score\n",
    "    \n",
    "    f1_macro = f1_score(true_labels, predicted_labels, average='macro')\n",
    "\n",
    "    print(f\"F1-score macro: {f1_macro:.2f}\")\n",
    "\n",
    "    \n",
    "    f1_micro = f1_score(true_labels, predicted_labels, average='micro')\n",
    "\n",
    "    print(f\"F1-score micro: {f1_micro:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer, DPRContextEncoder, DPRContextEncoderTokenizer\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import faiss\n",
    "\n",
    "# Load DPR question and context encoders\n",
    "question_encoder = DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\n",
    "question_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\n",
    "context_encoder = DPRContextEncoder.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')\n",
    "context_tokenizer = DPRContextEncoderTokenizer.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')\n",
    "\n",
    "# Encode the corpus using DPR context encoder\n",
    "corpus_embeddings = []\n",
    "for passage in train_df['Text']:\n",
    "    inputs = context_tokenizer(passage, return_tensors='pt', truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        embeddings = context_encoder(**inputs).pooler_output\n",
    "    corpus_embeddings.append(embeddings.squeeze().numpy())\n",
    "\n",
    "corpus_embeddings = torch.tensor(corpus_embeddings)\n",
    "\n",
    "index = faiss.IndexFlatIP(corpus_embeddings.shape[1])  # Using inner product (dot product) for similarity\n",
    "index.add(corpus_embeddings.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_examples(query, k=3):\n",
    "    # Encode the query using DPR question encoder\n",
    "    query_inputs = question_tokenizer(query, return_tensors='pt', truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        query_embedding = question_encoder(**query_inputs).pooler_output\n",
    "\n",
    "    for i in range(1,5000):\n",
    "\n",
    "        if k == 3:\n",
    "            pos = 1\n",
    "            neg = 1\n",
    "            neu = 1\n",
    "        \n",
    "            # Perform similarity search using FAISS\n",
    "            D, I = index.search(query_embedding.numpy(), k*i)\n",
    "\n",
    "            result = []\n",
    "\n",
    "            for idx in I[0]:\n",
    "                polarity = train_df['Polarity'][idx]\n",
    "                if polarity == 'positive' and pos!=0:\n",
    "                    result.append(idx)\n",
    "                    pos -= 1\n",
    "                elif polarity == 'negative' and neg!=0:\n",
    "                    result.append(idx)\n",
    "                    neg -= 1\n",
    "                elif polarity == 'neutral' and neu!=0:\n",
    "                    result.append(idx)\n",
    "                    neu -= 1\n",
    "        \n",
    "            if len(result) == 3:\n",
    "                break\n",
    "\n",
    "        if k==5:\n",
    "            pos = 2\n",
    "            neg = 2\n",
    "            neu = 2\n",
    "        \n",
    "            # Perform similarity search using FAISS\n",
    "            D, I = index.search(query_embedding.numpy(), k*i)\n",
    "\n",
    "            result = []\n",
    "\n",
    "            for idx in I[0]:\n",
    "                polarity = train_df['Polarity'][idx]\n",
    "                if polarity == 'positive' and pos!=0:\n",
    "                    result.append(idx)\n",
    "                    pos -= 1\n",
    "                elif polarity == 'negative' and neg!=0:\n",
    "                    result.append(idx)\n",
    "                    neg -= 1\n",
    "                elif polarity == 'neutral' and neu!=0:\n",
    "                    result.append(idx)\n",
    "                    neu -= 1\n",
    "        \n",
    "            if len(result) == 5:\n",
    "                break\n",
    "\n",
    "            if len(result) == 6:\n",
    "                result.pop()\n",
    "                break\n",
    "\n",
    "\n",
    "    return result\n",
    "\n",
    "# Example query\n",
    "query = \"Wow this code is great\"\n",
    "retrieved_examples = retrieve_relevant_examples(query, 5)\n",
    "print(\"Retrieved Examples:\")\n",
    "for i in retrieved_examples:\n",
    "    print()\n",
    "    print(train_df['Polarity'][i])\n",
    "    print(train_df['Text'][i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For 3 shot\n",
    "true_labels_DPR, predicted_labels_DPR, misclassifications_DPR = Generate_Few_Shot_llama(train_df, test_df, 3, retrieve_relevant_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Result(true_labels_DPR, predicted_labels_DPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For 5 shot\n",
    "true_labels_DPR_5, predicted_labels_DPR_5, misclassifications_DPR_5 = Generate_Few_Shot_llama(train_df, test_df, 5, retrieve_relevant_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Result(true_labels_DPR_5, predicted_labels_DPR_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the sentence transformer model for creating embeddings\n",
    "model_faiss = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Encode the corpus using the sentence transformer model\n",
    "corpus_embeddings_faiss = model_faiss.encode(train_df['Text'])\n",
    "\n",
    "# Use FAISS for efficient similarity search\n",
    "index_faiss = faiss.IndexFlatL2(corpus_embeddings_faiss.shape[1])  # Using L2 (Euclidean) distance\n",
    "index_faiss.add(corpus_embeddings_faiss)\n",
    "\n",
    "def retrieve_relevant_examples_faiss(query, k=3):\n",
    "    # Encode the query\n",
    "    query_embedding = model_faiss.encode([query])\n",
    "\n",
    "    for i in range(1,5000):\n",
    "\n",
    "        if k == 3:\n",
    "            pos = 1\n",
    "            neg = 1\n",
    "            neu = 1\n",
    "        \n",
    "            # Perform similarity search using FAISS\n",
    "            D, I = index_faiss.search(query_embedding, k*i)\n",
    "\n",
    "            result = []\n",
    "\n",
    "            for idx in I[0]:\n",
    "                polarity = train_df['Polarity'][idx]\n",
    "                if polarity == 'positive' and pos!=0:\n",
    "                    result.append(idx)\n",
    "                    pos -= 1\n",
    "                elif polarity == 'negative' and neg!=0:\n",
    "                    result.append(idx)\n",
    "                    neg -= 1\n",
    "                elif polarity == 'neutral' and neu!=0:\n",
    "                    result.append(idx)\n",
    "                    neu -= 1\n",
    "        \n",
    "            if len(result) == 3:\n",
    "                break\n",
    "\n",
    "        if k==5:\n",
    "            pos = 2\n",
    "            neg = 2\n",
    "            neu = 2\n",
    "        \n",
    "            # Perform similarity search using FAISS\n",
    "            D, I = index_faiss.search(query_embedding, k*i)\n",
    "\n",
    "            result = []\n",
    "\n",
    "            for idx in I[0]:\n",
    "                polarity = train_df['Polarity'][idx]\n",
    "                if polarity == 'positive' and pos!=0:\n",
    "                    result.append(idx)\n",
    "                    pos -= 1\n",
    "                elif polarity == 'negative' and neg!=0:\n",
    "                    result.append(idx)\n",
    "                    neg -= 1\n",
    "                elif polarity == 'neutral' and neu!=0:\n",
    "                    result.append(idx)\n",
    "                    neu -= 1\n",
    "        \n",
    "            if len(result) == 5:\n",
    "                break\n",
    "\n",
    "            if len(result) == 6:\n",
    "                result.pop()\n",
    "                break\n",
    "\n",
    "\n",
    "    return result\n",
    "\n",
    "# Example query\n",
    "query = \"You should change the for loop in lin 54\"\n",
    "retrieved_examples_faiss = retrieve_relevant_examples_faiss(query, 5)\n",
    "print(\"Retrieved Examples:\")\n",
    "for i in retrieved_examples_faiss:\n",
    "    print()\n",
    "    print(train_df['Polarity'][i])\n",
    "    print(train_df['Text'][i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For 3 shot\n",
    "true_labels_faiss_3, predicted_labels_faiss_3, misclassifications_faiss_3 = Generate_Few_Shot_llama(train_df, test_df, 3, retrieve_relevant_examples_faiss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Result(true_labels_faiss_3, predicted_labels_faiss_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For 3 shot\n",
    "true_labels_faiss_5, predicted_labels_faiss_5, misclassifications_faiss_5 = Generate_Few_Shot_llama(train_df, test_df, 5, retrieve_relevant_examples_faiss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Result(true_labels_faiss_5, predicted_labels_faiss_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Load SBERT model for creating embeddings\n",
    "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Encode the corpus using the sentence transformer model\n",
    "corpus_embeddings_nn = sbert_model.encode(train_df['Text'])\n",
    "\n",
    "neigh = NearestNeighbors(n_neighbors=5, metric='manhattan')\n",
    "neigh.fit(corpus_embeddings_nn)\n",
    "\n",
    "def retrieve_relevant_examples_nn(query, k=3):\n",
    "    # Encode the query using SBERT model\n",
    "    query_embedding = sbert_model.encode([query])\n",
    "\n",
    "    for i in range(1,5000):\n",
    "\n",
    "        if k == 3:\n",
    "            pos = 1\n",
    "            neg = 1\n",
    "            neu = 1\n",
    "        \n",
    "            # Perform similarity search using Manhattan\n",
    "            D, I = neigh.kneighbors(query_embedding, n_neighbors=k*i)\n",
    "\n",
    "            result = []\n",
    "\n",
    "            for idx in I[0]:\n",
    "                polarity = train_df['Polarity'][idx]\n",
    "                if polarity == 'positive' and pos!=0:\n",
    "                    result.append(idx)\n",
    "                    pos -= 1\n",
    "                elif polarity == 'negative' and neg!=0:\n",
    "                    result.append(idx)\n",
    "                    neg -= 1\n",
    "                elif polarity == 'neutral' and neu!=0:\n",
    "                    result.append(idx)\n",
    "                    neu -= 1\n",
    "        \n",
    "            if len(result) == 3:\n",
    "                break\n",
    "\n",
    "        if k==5:\n",
    "            pos = 2\n",
    "            neg = 2\n",
    "            neu = 2\n",
    "        \n",
    "            # Perform similarity search using Manhattan\n",
    "            D, I = neigh.kneighbors(query_embedding, n_neighbors=k*i)\n",
    "\n",
    "            result = []\n",
    "\n",
    "            for idx in I[0]:\n",
    "                polarity = train_df['Polarity'][idx]\n",
    "                if polarity == 'positive' and pos!=0:\n",
    "                    result.append(idx)\n",
    "                    pos -= 1\n",
    "                elif polarity == 'negative' and neg!=0:\n",
    "                    result.append(idx)\n",
    "                    neg -= 1\n",
    "                elif polarity == 'neutral' and neu!=0:\n",
    "                    result.append(idx)\n",
    "                    neu -= 1\n",
    "        \n",
    "            if len(result) == 5:\n",
    "                break\n",
    "\n",
    "            if len(result) == 6:\n",
    "                result.pop()\n",
    "                break\n",
    "\n",
    "\n",
    "    return result\n",
    "\n",
    "# Example query\n",
    "query = \"You should change the for loop in lin 54\"\n",
    "retrieved_examples_nn = retrieve_relevant_examples_nn(query, 5)\n",
    "print(\"Retrieved Examples:\")\n",
    "for i in retrieved_examples_nn:\n",
    "    print()\n",
    "    print(train_df['Polarity'][i])\n",
    "    print(train_df['Text'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For 3 shot\n",
    "true_labels_nn_3, predicted_labels_nn_3, misclassifications_nn_3 = Generate_Few_Shot_llama(train_df, test_df, 3, retrieve_relevant_examples_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Result(true_labels_nn_3, predicted_labels_nn_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For 5 shot\n",
    "true_labels_nn_5, predicted_labels_nn_5, misclassifications_nn_5 = Generate_Few_Shot_llama(train_df, test_df, 5, retrieve_relevant_examples_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Result(true_labels_nn_5, predicted_labels_nn_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# Tokenize the corpus\n",
    "tokenized_corpus = [doc.split(\" \") for doc in train_df['Text']]\n",
    "\n",
    "# Initialize BM25\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "def retrieve_relevant_examples_bm25(query, k=3):\n",
    "    # Tokenize the query\n",
    "    tokenized_query = query.split(\" \")\n",
    "    \n",
    "    # Get BM25 scores\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "    \n",
    "    for i in range(1,5000):\n",
    "\n",
    "        if k == 3:\n",
    "            pos = 1\n",
    "            neg = 1\n",
    "            neu = 1\n",
    "\n",
    "            last_num = k*i\n",
    "        \n",
    "            # Perform similarity search using Manhattan\n",
    "            I = scores.argsort()[-last_num:][::-1]\n",
    "\n",
    "            result = []\n",
    "\n",
    "            for idx in I:\n",
    "                polarity = train_df['Polarity'][idx]\n",
    "                if polarity == 'positive' and pos!=0:\n",
    "                    result.append(idx)\n",
    "                    pos -= 1\n",
    "                elif polarity == 'negative' and neg!=0:\n",
    "                    result.append(idx)\n",
    "                    neg -= 1\n",
    "                elif polarity == 'neutral' and neu!=0:\n",
    "                    result.append(idx)\n",
    "                    neu -= 1\n",
    "        \n",
    "            if len(result) == 3:\n",
    "                break\n",
    "\n",
    "        if k==5:\n",
    "            pos = 2\n",
    "            neg = 2\n",
    "            neu = 2\n",
    "\n",
    "            last_num = k*i\n",
    "        \n",
    "            # Perform similarity search using Manhattan\n",
    "            I = scores.argsort()[-last_num:][::-1]\n",
    "\n",
    "            result = []\n",
    "\n",
    "            for idx in I:\n",
    "                polarity = train_df['Polarity'][idx]\n",
    "                if polarity == 'positive' and pos!=0:\n",
    "                    result.append(idx)\n",
    "                    pos -= 1\n",
    "                elif polarity == 'negative' and neg!=0:\n",
    "                    result.append(idx)\n",
    "                    neg -= 1\n",
    "                elif polarity == 'neutral' and neu!=0:\n",
    "                    result.append(idx)\n",
    "                    neu -= 1\n",
    "        \n",
    "            if len(result) == 5:\n",
    "                break\n",
    "\n",
    "            if len(result) == 6:\n",
    "                result.pop()\n",
    "                break\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# Example query\n",
    "query = \"You should change the for loop in lin 54\"\n",
    "retrieved_examples_bm25 = retrieve_relevant_examples_bm25(query, 5)\n",
    "print(\"Retrieved Examples:\")\n",
    "for i in retrieved_examples_bm25:\n",
    "    print()\n",
    "    print(train_df['Polarity'][i])\n",
    "    print(train_df['Text'][i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For 3 shot\n",
    "true_labels_bm25, predicted_labels_bm25, misclassifications_bm25 = Generate_Few_Shot_llama(train_df, test_df, 3, retrieve_relevant_examples_bm25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Result(true_labels_bm25, predicted_labels_bm25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For 3 shot\n",
    "true_labels_bm25_5, predicted_labels_bm25_5, misclassifications_bm25_5 = Generate_Few_Shot_llama(train_df, test_df, 5, retrieve_relevant_examples_bm25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Result(true_labels_bm25_5, predicted_labels_bm25_5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
